---
title: "hog"
author: "Jiaqi Dong jd3418"
date: "2018/3/18"
output: html_document
---

```{r}
# Load packages
#install.packages("randomForest")
#install.packages("caret")
#install.packages("e1071")
#install.packages("e1071")

library(ggplot2)
library(randomForest)
library(caret)
library(e1071)
library(knitr)
```

```{r}
# Controls
K <- 5 # number of CV folds
proportion = 0.8 # training set proportion
seed = 123 # set seed
```

```{r}
# Import and split the data into test and training set (80% training set and 20% test set)
setwd("/Users/Dongjiaqi/Desktop/5243")
features <- read.csv("hog_feature.csv",header=T,as.is=TRUE)
label.train <- read.csv("label_train.csv",header=TRUE,as.is=TRUE)[,3]


n <- dim(features)[1]
set.seed(seed)
index <- sample(n, n*proportion, replace = FALSE)

x.train <- features[index,]
y.train <- label.train[index]

x.test <- features[-index,]
y.test <- label.train[-index]
```

```{r}
# Model selection with cross-validation: choosing different values of training model parameters
m <- length(y.train)
m.fold <- floor(m/K)
set.seed(seed)
s <- sample(rep(1:K, c(rep(m.fold, K-1), m-(K-1)*m.fold))) 

cv.error <- rep(NA, K)
opt.mtry <- rep(NA,K) # optimal mtry for certain fold training data

n_trees <- seq(500,1000,100)
cv.ntree.error <- rep(NA, length(n_trees)) # lowest cv-error for certain ntree value
best.ntree.mtry <- rep(NA, length(n_trees)) # best mtry for certain ntree value

for (j in 1:length(n_trees)){
  for (i in 1:K){
    train.data <- x.train[s != i,]
    train.label <- y.train[s != i]
    test.data <- x.train[s == i,]
    test.label <- y.train[s == i]
  
    fit <- tuneRF(train.data, as.factor(train.label),
                mtreeTry = n_trees[j], 
                doBest = TRUE)
 
    # Get the 'mtry' for trained model
    opt.mtry[i] <- fit$mtry
    pred <- predict(fit, test.data)
    cv.error[i] <- mean(pred != test.label)
  }
  
  # Get the lowest error rate of cross validation
  cv.ntree.error[j] <- min(cv.error)
  best.ntree.mtry[j] <- opt.mtry[which.min(cv.error)]
}
```


```{r}
# Evaluation
# Visualize Cross Validation
ggplot(data = data.frame(cv.ntree.error)) + geom_point(aes(x = n_trees, y = cv.ntree.error), color = "blue")

# Get the best parameters
best <- which.min(cv.ntree.error)
best.ntree <- n_trees[best]
best.mtry <- best.ntree.mtry[best]

# Training error
tm_train<-system.time(fit.1 <- randomForest(x.train, as.factor(y.train), 
                                  mtry = best.mtry, ntree = best.ntree, importance = TRUE))

train_error <- mean(fit.1$predicted != y.train)
train_error
save(fit.1, file="RFs_fit_hog.RData")

# Test error
tm_test<-system.time(test_pred <- predict(fit.1, x.test))
test_error <- mean(test_pred != y.test)
test_error 
save(test_pred, file="pred_test_RF_hog.RData")

### Summarize Running Time
cat("Time for training model=", tm_train[1], "s \n") 
cat("Time for testing model=", tm_test[1], "s \n") 
```
